# lightning.pytorch==2.0.5
seed_everything: 2144748874
trainer:
  target_batch_size: 32
  microbatch_size: 1
  num_nodes: 1
  devices: auto
  accelerator: gpu
  strategy: deepspeed_stage_2_offload
  precision: bf16
  callbacks:
  - class_path: custom.SaveElementWeightsCallback  # Replace with the actual module path
    init_args:
      save_dir: checkpoint  # Use the YAML's checkpoint directory
      elements_to_save:
        - lora  # Example element to save (replace with actual element names)
        - emb  # Example element to save (replace with actual element names)
        - head # Example element to save (replace with actual element names)
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      name: null
      save_dir: .
      version: null
      offline: false
      dir: null
      id: null
      anonymous: null
      project: RWKV6-infctx-LoRA
      log_model: false
      experiment: null
      prefix: ''
      checkpoint_name: null
      job_type: null
      config: null
      entity: null
      reinit: null
      tags:
      - RWKV
      group: null
      notes: null
      magic: null
      config_exclude_keys: null
      config_include_keys: null
      mode: null
      allow_val_change: null
      resume: null
      force: null
      tensorboard: null
      sync_tensorboard: null
      monitor_gym: null
      save_code: null
      settings: null
  fast_dev_run: false
  max_epochs: 100
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: null
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 0
  log_every_n_steps: null
  enable_checkpointing: false
  enable_progress_bar: null
  enable_model_summary: null
  accumulate_grad_batches: 1
  gradient_clip_val: null
  gradient_clip_algorithm: null
  deterministic: null
  benchmark: null
  inference_mode: true
  use_distributed_sampler: false
  profiler: null
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: null
model:
  load_model: rwkv-x060-14b-world-v2.1-26%trained-20240501-ctx4k.pth
  n_embd: -1
  n_layer: -1
  vocab_size: -1
  ctx_len: 16384
  ctx_len_cutoffs: []
  ctx_len_warmup_steps: []
  lora_r: 16.0 # LoRA Rank
  lora_alpha: 32.0 #LoRA Alpha it must be double LoRA Rank
  lora_dropout: 0.01
  lora_quant: true
  lora_quant_type: nf4
  head_learning: true #if true Enable Full-resolution Head Layer Learning 
  lr_init: 1.0e-05
  lr_final: 1.0e-07
  lr_period: -1
  lr_period_type: epoch
  dropout: 0.0
  beta1: 0.9
  beta2: 0.99
  adam_eps: 1.0e-08
  weight_decay: 0.01
  warmup_steps: -1
  position_loss_bias: 1.0
  position_loss_bias_in_validation: false
  grad_cp: true
  bptt_learning: true
  bptt_learning_range: -1
  bptt_truncated_learning: false
  layerwise_lr: true
  dim_att: null
  dim_ffn: null
  substep_cuda_cache_clear: false
  substep_logging: false
  torch_set_float32_matmul_precision: high
data:
  data_path: dataset/
  source: json
  source_data_dir: dataset_src/
  source_dataset_params: null
  test_split: 0.01
  test_split_shuffle: true
  text_rechunk_size: 2048
  text_rechunk_auto: true
  text_rechunk_force: false
  tokenizer: world
  autoTokenizer: null
  world_add_endoftext_token: true
  min_token_size: 1
  max_token_size: -1
  sort_by_length: false
  sort_asc: false
  training_dataloader_shuffle_auto: true
  dataset_offset: -1.0
  dataset_length: -1.0
  custom_text_key: null
  multi_column_keys: null
  multi_column_prefix: null
  multi_column_suffix: null
  multi_column_train_mask: null
  multi_column_separator: null
  conversation_format: null
  conversation_key: null
  conversation_input_key_prefix_map: null
  conversation_input_key_mask: null
  conversation_sender_key: null
  conversation_sender_value_map: null
  conversation_input_key_map: null
  conversation_sender_suffix: null
  conversation_sender_mask: null
  conversation_end_of_conversation: null
  disable_prompt_completion_mask: false
  packing_enable: true
  packing_batchsize: 20160
  packing_chunksize: 4096
  packing_min_ctx_len: -1
  packing_in_sequence: false
  processing_max_batch_size: 100000
  skip_datapath_setup: false
ckpt_path: null
